\section{Examples}
\label{sec:examples}


\subsection{Floating Point Operations}

Principal Components Analysis is a common statistical analysis technique.  The 
implementation of PCA typically involves computing the singular value 
decomposition (SVD) of the input data and then projecting the data onto the 
right singular vectors.  It is known that an SVD requires $6mn^2 + 20n^3$ 
floating point operations and that the projection onto the right singular 
vectors requires an additional $2mn^2$ operations~\citep{gvl}.  Finally, as PCA 
is usually performed on centered (and often scaled) data, we require an 
additional $2*mn+1$ operations for centering the data.

The \thispackage package contains the example \code{pca.r} as a package demo, 
which will perform a PCA on random data consisting of $10,000$ observations and 
50 predictors.  The analysis uses \R's ordinary \code{prcomp()} routine, and 
the performance is measured by \thispackage's \code{system.flops()}.  You can 
run this demo by calling

\begin{lstlisting}[language=rr]
demo("pca", package="pbdPAPI")
\end{lstlisting}


An example output from this machine is:

\begin{Output}
      m  n  measured theoretical difference pct.error   mflops
1 10000 50 212538720   203500001    9038719   4.25274 2284.257
\end{Output}

The \thispackage package has several other demonstrations that, like this, 
compare a theoretical floating point operations count against a measured count, 
and then display the Mflops the computation achieved.  These demos include 
an inner product calculation \code{inner.r}, a matrix-matrix product 
\code{matprod.r}, and the fitting of a linear model in \code{regression.r}.


\subsection{Cache Misses}

To see the full source code described in this example, see the \code{cache_access} demo in 
\thispackage.

Consider the following example, where we will fill a matrix with 1's, first by 
looping over rows then columns, and then by looping over columns then rows.  For 
maximum effect, we will be dropping to \CXX by way of \pkg{Rcpp}.  If you do not 
have \pkg{Rcpp} installed on your system, you can still follow along (even if 
you don't know \CXX), but you will not be able to recreate the timings locally.

You are probably aware that \R matrices are stored in column-major fashion.  
What this means is that the data, as it is laid out in physical memory, is 
easier to go from the entry with index $(i, j)$ to index $(i+1, j)$ than 
it is to go to index $(i, j+1)$.  

An example of accessing data poorly is the following:
\begin{lstlisting}[language=rr]
bad_cache_access <- "
  int i, j;
  const int n = INTEGER(n_)[0];
  Rcpp::NumericMatrix x(n, n);
  
  
  for (i=0; i<n; i++)
    for (j=0; j<n; j++)
      x(i, j) = 1.;
  
  return x;
"
\end{lstlisting}

Accessing data in this way maximizes the 
amount of work the computer needs to do in searching for the data your program 
needs.  Accessing your memory correctly will help minimize cache misses: 

\begin{lstlisting}[language=rr]
good_cache_access <- "
  int i, j;
  const int n = INTEGER(n_)[0];
  Rcpp::NumericMatrix x(n, n);
  
  
  for (j=0; j<n; j++)
    for (i=0; i<n; i++)
      x(i, j) = 1.;
  
  return x;
"
\end{lstlisting}

To prove this, we can build these functions using the \pkg{inline} and 
\pkg{Rcpp} packages~\citep{inline,rcpp} and then profile them with \thispackage.

\begin{lstlisting}[language=rr]
library(inline)

bad <- cxxfunction(signature(n_="integer"), 
                   body=bad_cache_access, plugin="Rcpp")
good <- cxxfunction(signature(n_="integer"), 
                    body=good_cache_access, plugin="Rcpp")
\end{lstlisting}

A quick check of run times shows something drastically different is happening 
between the two implementations:
\begin{Output}
n <- 10000L

system.time(bad(n))
#   user  system elapsed 
#  1.016   0.232   1.259 

system.time(good(n))
#   user  system elapsed 
#  0.201   0.155   0.357 
\end{Output}

So even though we (mathematically) are doing the exact same thing to the data, 
the run times differ by a factor of 3.5.  \thispackage allows us to more 
thoroughly see what's happening.  We can use \code{system.cache()} to check the 
L1, L2, and L3 (total) cache misses for each of these functions:

\begin{Output}
library(pbdPAPI)
n <- 10000L

system.cache(bad(n))
#$L1.total
#[1] 193580295
#
#$L2.total
#[1] 159442230
#
#$L3.total
#[1] 16895275

system.cache(good(n))
#$L1.total
#[1] 15552007
#
#$L2.total
#[1] 11580023
#
#$L3.total
#[1] 801150
\end{Output}

The L1 cache misses differ by more than an order of magnitude, 194 million to 16 
million!

Perhaps a more useful measure is the \emph{cache miss ratio}, which is the 
total cache misses divided by the total cache accesses (in the low-level 
interface syntax, this is the return of event \code{PAPI_L2_TCM} divided by the 
return of event \code{PAPI_L2_TCA}, for level 2). 

Measuring this too is simple:

\begin{Output}
system.cache(bad(n), events="l2.ratio")
# L2 cache miss ratio 
#            0.815597 

system.cache(good(n), events="l2.ratio")
# L2 cache miss ratio 
#           0.7156862 

\end{Output}
