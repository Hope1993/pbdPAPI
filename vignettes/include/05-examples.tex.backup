\section{Examples}


\subsection{Floating Point Operations}

Principal Components Analysis is a common statistical analysis technique.  The 
implementation of PCA typically involves computing the singular value 
decomposition (SVD) of the input data and then projecting the data onto the 
right singular vectors.  It is known that an SVD requires $6mn^2 + 20n^3$ 
floating point operations and that the projection onto the right singular 
vectors requires an additional $2mn^2$ operations~\citep{gvl}.  Finally, as PCA 
is usually performed on centered (and often scaled) data, we require an 
additional $2*mn+1$ operations for centering the data.

The \thispackage package contains the example \code{pca.r} as a package demo, 
which will perform a PCA on random data consisting of $10,000$ observations and 
50 predictors.  The analysis uses \R's ordinary \code{prcomp()} routine, and 
the performance is measured by \thispackage's \code{system.flops()}.  You can 
run this demo by calling

\begin{lstlisting}[language=rr]
demo("pca", package="pbdPAPI")
\end{lstlisting}


An example output from this machine is:

\begin{Output}
      m  n  measured theoretical difference pct.error   mflops
1 10000 50 212538720   203500001    9038719   4.25274 2284.257
\end{Output}

The \thispackage package has several other demonstrations that, like this, 
compare a theoretical floating point operations count against a measured count, 
and then display the Mflops the operation achieved.  These demos include 
an inner product calculation \code{inner.r}, a matrix-matrix product 
\code{matprod.r}, and the fitting of a linear model in \code{regression.r}.


\subsection{Cache Misses}

To see the full source code described in this example, see the \code{cache_access} demo in 
\thispackage.

Consider the following example, where we will fill a matrix with 1's, first by 
looping over rows then columns, and then by looping over columns then rows.  For 
maximum effect, we will be dropping to \C by way of \pkg{Rcpp}.  If you do not 
have \pkg{Rcpp} installed on your system, you can still follow along (even if 
you don't know \CXX), but you will not be able to recreate the timings locally.

You are probably aware that \R matrices are stored in column-major fashion.  
What this means is that the data, as it is laid out in physical memory, is 
easier to go from the entry with index $(i, j)$ to index $(i+1, j)$ than 
it is to go to index $(i, j+1)$.  Accessing in the correct way minimizes the 
amount of work the computer needs to do in searching for the data your program 
needs.  And in particular, this will help minimize cache misses, because if you 
routinely access data 
\begin{lstlisting}[language=rr]
bad_cache_access <- "
  int i, j;
  const int n = INTEGER(n_)[0];
  Rcpp::NumericMatrix x(n, n);
  
  
  for (i=0; i<n; i++)
    for (j=0; j<n; j++)
      x(i, j) = 1.;
  
  return x;
"
\end{lstlisting}

\begin{lstlisting}[language=rr]
good_cache_access <- "
  int i, j;
  const int n = INTEGER(n_)[0];
  Rcpp::NumericMatrix x(n, n);
  
  
  for (j=0; j<n; j++)
    for (i=0; i<n; i++)
      x(i, j) = 1.;
  
  return x;
"
\end{lstlisting}

\begin{lstlisting}[language=rr]
library(inline)

bad <- cxxfunction(signature(n_="integer"), 
                   body=bad_cache_access, plugin="Rcpp")
good <- cxxfunction(signature(n_="integer"), 
                    body=good_cache_access, plugin="Rcpp")
\end{lstlisting}

A quick check of run times shows something drastically different happening:
\begin{Output}
n <- 10000L

system.time(bad(n))
#   user  system elapsed 
#  1.016   0.232   1.259 

system.time(good(n))
#   user  system elapsed 
#  0.201   0.155   0.357 
\end{Output}

So even though we (mathematically) are doing the exact same thing, the run 
times differ by a factor of 3.5.  \thispackage allows us to more thoroughly see 
what's happening.  If we use \code{system.cache()} to check the L1, L2, and L3 
cache misses for each of these functions:
\begin{Output}
library(pbdPAPI)
n <- 10000L

system.cache(bad(n))
#$L1.total
#[1] 193580295
#
#$L2.total
#[1] 159442230
#
#$L3.total
#[1] 16895275

system.cache(good(n))
#$L1.total
#[1] 15552007
#
#$L2.total
#[1] 11580023
#
#$L3.total
#[1] 801150
\end{Output}

it should be readily apparent what is going on now.  The L1 cache misses differ 
by more than an order of magnitude, 194 million to 16 million!


\begin{Output}
system.cache(bad(n), events="l2.ratio")
# L2 cache miss ratio 
#            0.815597 

system.cache(good(n), events="l2.ratio")
# L2 cache miss ratio 
#           0.7156862 

\end{Output}

